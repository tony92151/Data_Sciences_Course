{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport sys\nimport time\nimport numpy as np\nimport math\nimport pandas as pd\nfrom PIL import Image\nfrom datetime import datetime\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport random\nimport datetime\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(os.listdir(\"../input/imet-2019-fgvc6/train\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/imet-2019-fgvc6/train.csv\")\nlable = pd.read_csv(\"../input/imet-2019-fgvc6/labels.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lable_length = len(lable)\ntrain_length = len(train)\ntrain_length\nlable_length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def creatData(train,lable_length):\n    train = np.array(train)\n    train_data = []\n    for t in range(train_length):\n        v = np.zeros(lable_length)\n        #print(train[t,1])\n        for s in train[t,1].split(\" \"):\n            #print(s)\n            v[int(s)] = 1\n        train_data.append([train[t,0],v])\n    return np.array(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_lib = creatData(train,lable_length)\nprint(train_lib)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transformer = transforms.Compose([\n  transforms.Resize((128,128)),              # resize the image to \n  transforms.RandomHorizontalFlip(),  # randomly flip image horizontally\n  transforms.ToTensor()])             # transform it into a PyTorch Tensor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class trainDataset(Dataset):\n    def __init__(self, train_lib, transform=None):\n        self.filenames = train_lib[:,0]\n        self.labels = train_lib[:,1]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        image = Image.open(\"../input/imet-2019-fgvc6/train/\"+format(self.filenames[idx])+'.png')  # PIL image\n        image = self.transform(image)\n        return image, self.labels[idx]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataloader = DataLoader(trainDataset(train_lib, train_transformer), \n                              batch_size=32, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(np.array(train_dataloader.dataset[4][0]).shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class InceptionA(nn.Module):\n    def __init__(self,in_channels):\n        super(InceptionA, self).__init__()\n        self.conv1 = nn.Sequential(       # input  (3,_,_)\n            nn.AvgPool2d(3,1,1),          # output  (3,_,_)\n            nn.Conv2d(in_channels,16,1),  # output  (16,_,_)\n        )\n        self.conv2 = nn.Sequential(       # input  (3,_,_)\n            nn.MaxPool2d(3,1,1),          # output  (3,_,_)\n            nn.Conv2d(in_channels,16,1),  # output  (16,_,_)\n        )\n        \n        self.conv3 = nn.Sequential(       # input  (3,_,_)\n            nn.Conv2d(in_channels,16,1),  # output  (16,_,_)\n            nn.Conv2d(16,16,5,1,2),       # output  (16,_,_)\n        )\n        \n        self.conv4 = nn.Sequential(       # input  (3,_,_)\n            nn.Conv2d(in_channels,8,1),   # output  (8,_,_)\n            #nn.Conv2d(16,24,3,1,1),      # output  (24,_,_)\n            nn.Conv2d(8,16,3,1,1),        # output  (16,_,_)\n        )\n        \n    def forward(self, x):\n        x1 = self.conv1(x)\n        x2 = self.conv2(x)\n        x3 = self.conv3(x)\n        x4 = self.conv4(x)\n        \n        outputs = [x2,x3,x4,x1]\n        #outputs = [x3,x4,x1]#72\n        \n        return torch.cat(outputs,1)       # output  (16*4, 32, 32)<<\n                                          # output  (88, 32, 32)<<\n####################################################################\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Sequential(       # input  (3, 32, 32)\n            nn.Conv2d(3,20,5,1,2),        # output  (20, 32, 32)\n            InceptionA(in_channels=20),   # output  (64, 32, 32)\n            nn.MaxPool2d(4),              # output  (64, 16, 16)\n        )\n        self.conv2 = nn.Sequential(       # input  (64, 16, 16)\n            nn.Conv2d(64,40,5,1,2),       # output  (40, 16, 16)\n            InceptionA(in_channels=40),   # output  (64, 16, 16)\n            nn.MaxPool2d(2),              # output  (64, 8, 8)\n        )\n        \n        self.out1 = nn.Linear(64*16*16, 64*16*8) #fully connected layer\n        self.out2 = nn.Linear(64*16*8, 64*16*4)\n        self.out3 = nn.Linear(64*16*4, 1103)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        #x = x.view(-1, 32 * 8 * 8)\n        x = x.view(x.size(0), -1)\n        x = self.out1(x)\n        x = self.out2(x)\n        x = self.out3(x)\n        #output = nn.functional.log_softmax(x, dim=1)\n        return x    # return x for visualization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn = CNN()\ncnn.cuda()\nprint(cnn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\nloss_func = torch.nn.MSELoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(epoch):\n    for step, (x, y) in enumerate(train_dataloader):\n        data = Variable(x).cuda()   # batch x\n        target = Variable(y).cuda()   # batch y\n        #print(data.type())\n        #print(target.type())\n        output = cnn(data)               # cnn output\n        #loss = nn.functional.nll_loss(output, target)\n        loss = loss_func(output, target.float())   # cross entropy loss\n        optimizer.zero_grad()           # clear gradients for this training step\n        loss.backward()                 # backpropagation, compute gradients\n        optimizer.step()                # apply gradients\n\n        if step % 100 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, \n           step * len(data), len(train_dataloader.dataset),100. * step / len(train_dataloader), loss.data.item()))\n    print(\"Finish\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(2):\n    train(epoch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(cnn, 'net.pkl')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}