{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dsai2-1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "tIXoKKUi91qi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "#torch.set_default_tensor_type('torch.cuda.FloatTensor')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WhzPO7NRo42i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 數據產生器，亂數產生出\"100+150 \"及 \"250 \"等數據"
      ]
    },
    {
      "metadata": {
        "id": "9ayrHp8gS71r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def creatData(length):\n",
        "  que = []\n",
        "  ans = []\n",
        "  for i in range(length):\n",
        "    x = random.randint(1,1000)\n",
        "    y = random.randint(1,999)\n",
        "    a = x+y\n",
        "    str1 = str(x)+'+'+str(y)\n",
        "    str2 = str(a)\n",
        "    while len(str1)<8: \n",
        "      str1 = str1+' '\n",
        "    while len(str2)<4: \n",
        "      str2 = str2+' '\n",
        "    que.append(str1)\n",
        "    ans.append(str2)\n",
        "  return que,ans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v13Ksupd2yJb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dict1 = { ' ': [1,0,0,0,0,0,0,0,0,0,0,0],\n",
        "         '+': [0,1,0,0,0,0,0,0,0,0,0,0],\n",
        "         '0': [0,0,1,0,0,0,0,0,0,0,0,0],\n",
        "         '1': [0,0,0,1,0,0,0,0,0,0,0,0],\n",
        "         '2': [0,0,0,0,1,0,0,0,0,0,0,0],\n",
        "         '3': [0,0,0,0,0,1,0,0,0,0,0,0],\n",
        "         '4': [0,0,0,0,0,0,1,0,0,0,0,0],\n",
        "         '5': [0,0,0,0,0,0,0,1,0,0,0,0],\n",
        "         '6': [0,0,0,0,0,0,0,0,1,0,0,0],\n",
        "         '7': [0,0,0,0,0,0,0,0,0,1,0,0],\n",
        "         '8': [0,0,0,0,0,0,0,0,0,0,1,0],\n",
        "         '9': [0,0,0,0,0,0,0,0,0,0,0,1]}\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "etRljMOYposK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 將文字依照上方字典做 Feature Engineering"
      ]
    },
    {
      "metadata": {
        "id": "HjIHxxyZyexQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ceratDataSet(que,ans):\n",
        "  #global = dict\n",
        "  Q,A = [],[]\n",
        "  for i in range(len(que)):\n",
        "    a,b = [],[]\n",
        "    for n in range(len(que[0])):\n",
        "      a1 = dict1[que[i][n]]\n",
        "      a = a+a1\n",
        "    for k in range(len(ans[0])):\n",
        "      a2 = dict1[ans[i][k]]\n",
        "      b = b+a2\n",
        "    Q.append(a)\n",
        "    A.append(b)\n",
        "  return np.array(Q),np.array(A)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PV2yzIq_pyQG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# pytorch數據集"
      ]
    },
    {
      "metadata": {
        "id": "r1odkWaLPDVV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, qua, ans):\n",
        "    self.Q = qua\n",
        "    self.A = ans\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.A)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      q = self.Q[idx]\n",
        "      a = self.A[idx]\n",
        "      return q,a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hscb5zVp4o8m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "que,ans = creatData(400000)\n",
        "\n",
        "persentage = 0.8\n",
        "\n",
        "Q,A = ceratDataSet(que,ans)\n",
        "fron = int(len(Q)*persentage)\n",
        "\n",
        "\n",
        "Q_train = Q[:fron]\n",
        "A_train = A[:fron]\n",
        "\n",
        "Q_test = Q[fron:]\n",
        "A_test = A[fron:]\n",
        "\n",
        "train_dataloader = DataLoader(MyDataset(Q_train,A_train),batch_size=100, shuffle=True)\n",
        "test_dataloader = DataLoader(MyDataset(Q_test,A_test), shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7AONQ83fse1Q",
        "colab_type": "code",
        "outputId": "ce647b67-7045-4ef8-a5d0-ce581a626fd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "use_gpu = torch.cuda.is_available()\n",
        "print(use_gpu)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CuwoJhobp8OQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 定義NN，這邊用6層 linear connection 做連接"
      ]
    },
    {
      "metadata": {
        "id": "6ELVjLC5-o17",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.out1 = torch.nn.Linear(12*8,120)\n",
        "    self.out2 = torch.nn.Linear(120,180)\n",
        "    self.out3 = torch.nn.Linear(180,300)\n",
        "    self.out4 = torch.nn.Linear(300,400)\n",
        "    self.out5 = torch.nn.Linear(400,150)\n",
        "    self.out6 = torch.nn.Linear(150,12*4)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print(self.out1.weight.type())\n",
        "    #print(x.shape)\n",
        "    x = self.out1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.out2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.out3(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.out4(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.out5(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.out6(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hghwnMkHvyGs",
        "colab_type": "code",
        "outputId": "1b38d622-4e90-4034-ea74-678f91a3c082",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "cell_type": "code",
      "source": [
        "model = CNN()\n",
        "\n",
        "print(type(model))\n",
        "\n",
        "if use_gpu:\n",
        "  model = model.cuda()\n",
        "print(model)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class '__main__.CNN'>\n",
            "CNN(\n",
            "  (out1): Linear(in_features=96, out_features=120, bias=True)\n",
            "  (out2): Linear(in_features=120, out_features=180, bias=True)\n",
            "  (out3): Linear(in_features=180, out_features=300, bias=True)\n",
            "  (out4): Linear(in_features=300, out_features=400, bias=True)\n",
            "  (out5): Linear(in_features=400, out_features=150, bias=True)\n",
            "  (out6): Linear(in_features=150, out_features=48, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jutQad1vyX6i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCH = 100\n",
        "\n",
        "loss_func = torch.nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0015)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kpXXWJStqM0N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 訓練 50 EPOCH"
      ]
    },
    {
      "metadata": {
        "id": "w1f_XGw-whoF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  for step, (x, y) in enumerate(train_dataloader):\n",
        "      data = x.type(torch.cuda.FloatTensor)\n",
        "      target = y.type(torch.cuda.FloatTensor)\n",
        "\n",
        "      output = model(data)               # cnn output\n",
        "\n",
        "      loss = loss_func(output, target)   # cross entropy loss\n",
        "      optimizer.zero_grad()           # clear gradients for this training step\n",
        "      loss.backward()                 # backpropagation, compute gradients\n",
        "      optimizer.step()                # apply gradients\n",
        "\n",
        "      if step % 2000 == 0:\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, \n",
        "         step * len(data), len(train_dataloader.dataset),100. * step / len(train_dataloader), loss.data.item()))\n",
        "  print(\"Finish\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bU8GmsOPR1DJ",
        "colab_type": "code",
        "outputId": "a3303ea3-8128-43e7-b468-d337c9cb63e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5244
        }
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(1,EPOCH):\n",
        "    train(epoch)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/320000 (0%)]\tLoss: 0.083961\n",
            "Train Epoch: 1 [200000/320000 (62%)]\tLoss: 0.037595\n",
            "Finish\n",
            "Train Epoch: 2 [0/320000 (0%)]\tLoss: 0.027650\n",
            "Train Epoch: 2 [200000/320000 (62%)]\tLoss: 0.020848\n",
            "Finish\n",
            "Train Epoch: 3 [0/320000 (0%)]\tLoss: 0.016399\n",
            "Train Epoch: 3 [200000/320000 (62%)]\tLoss: 0.009630\n",
            "Finish\n",
            "Train Epoch: 4 [0/320000 (0%)]\tLoss: 0.006406\n",
            "Train Epoch: 4 [200000/320000 (62%)]\tLoss: 0.004313\n",
            "Finish\n",
            "Train Epoch: 5 [0/320000 (0%)]\tLoss: 0.004767\n",
            "Train Epoch: 5 [200000/320000 (62%)]\tLoss: 0.002722\n",
            "Finish\n",
            "Train Epoch: 6 [0/320000 (0%)]\tLoss: 0.000726\n",
            "Train Epoch: 6 [200000/320000 (62%)]\tLoss: 0.001231\n",
            "Finish\n",
            "Train Epoch: 7 [0/320000 (0%)]\tLoss: 0.001023\n",
            "Train Epoch: 7 [200000/320000 (62%)]\tLoss: 0.000892\n",
            "Finish\n",
            "Train Epoch: 8 [0/320000 (0%)]\tLoss: 0.000952\n",
            "Train Epoch: 8 [200000/320000 (62%)]\tLoss: 0.000907\n",
            "Finish\n",
            "Train Epoch: 9 [0/320000 (0%)]\tLoss: 0.001585\n",
            "Train Epoch: 9 [200000/320000 (62%)]\tLoss: 0.001370\n",
            "Finish\n",
            "Train Epoch: 10 [0/320000 (0%)]\tLoss: 0.000609\n",
            "Train Epoch: 10 [200000/320000 (62%)]\tLoss: 0.000796\n",
            "Finish\n",
            "Train Epoch: 11 [0/320000 (0%)]\tLoss: 0.001544\n",
            "Train Epoch: 11 [200000/320000 (62%)]\tLoss: 0.001374\n",
            "Finish\n",
            "Train Epoch: 12 [0/320000 (0%)]\tLoss: 0.001972\n",
            "Train Epoch: 12 [200000/320000 (62%)]\tLoss: 0.001381\n",
            "Finish\n",
            "Train Epoch: 13 [0/320000 (0%)]\tLoss: 0.000914\n",
            "Train Epoch: 13 [200000/320000 (62%)]\tLoss: 0.000439\n",
            "Finish\n",
            "Train Epoch: 14 [0/320000 (0%)]\tLoss: 0.001222\n",
            "Train Epoch: 14 [200000/320000 (62%)]\tLoss: 0.000863\n",
            "Finish\n",
            "Train Epoch: 15 [0/320000 (0%)]\tLoss: 0.001303\n",
            "Train Epoch: 15 [200000/320000 (62%)]\tLoss: 0.000649\n",
            "Finish\n",
            "Train Epoch: 16 [0/320000 (0%)]\tLoss: 0.001127\n",
            "Train Epoch: 16 [200000/320000 (62%)]\tLoss: 0.000302\n",
            "Finish\n",
            "Train Epoch: 17 [0/320000 (0%)]\tLoss: 0.002059\n",
            "Train Epoch: 17 [200000/320000 (62%)]\tLoss: 0.000585\n",
            "Finish\n",
            "Train Epoch: 18 [0/320000 (0%)]\tLoss: 0.001887\n",
            "Train Epoch: 18 [200000/320000 (62%)]\tLoss: 0.000646\n",
            "Finish\n",
            "Train Epoch: 19 [0/320000 (0%)]\tLoss: 0.000215\n",
            "Train Epoch: 19 [200000/320000 (62%)]\tLoss: 0.000836\n",
            "Finish\n",
            "Train Epoch: 20 [0/320000 (0%)]\tLoss: 0.000437\n",
            "Train Epoch: 20 [200000/320000 (62%)]\tLoss: 0.000644\n",
            "Finish\n",
            "Train Epoch: 21 [0/320000 (0%)]\tLoss: 0.001869\n",
            "Train Epoch: 21 [200000/320000 (62%)]\tLoss: 0.002166\n",
            "Finish\n",
            "Train Epoch: 22 [0/320000 (0%)]\tLoss: 0.001013\n",
            "Train Epoch: 22 [200000/320000 (62%)]\tLoss: 0.000426\n",
            "Finish\n",
            "Train Epoch: 23 [0/320000 (0%)]\tLoss: 0.000740\n",
            "Train Epoch: 23 [200000/320000 (62%)]\tLoss: 0.000392\n",
            "Finish\n",
            "Train Epoch: 24 [0/320000 (0%)]\tLoss: 0.000619\n",
            "Train Epoch: 24 [200000/320000 (62%)]\tLoss: 0.000725\n",
            "Finish\n",
            "Train Epoch: 25 [0/320000 (0%)]\tLoss: 0.000540\n",
            "Train Epoch: 25 [200000/320000 (62%)]\tLoss: 0.000582\n",
            "Finish\n",
            "Train Epoch: 26 [0/320000 (0%)]\tLoss: 0.001139\n",
            "Train Epoch: 26 [200000/320000 (62%)]\tLoss: 0.001120\n",
            "Finish\n",
            "Train Epoch: 27 [0/320000 (0%)]\tLoss: 0.001343\n",
            "Train Epoch: 27 [200000/320000 (62%)]\tLoss: 0.000440\n",
            "Finish\n",
            "Train Epoch: 28 [0/320000 (0%)]\tLoss: 0.000880\n",
            "Train Epoch: 28 [200000/320000 (62%)]\tLoss: 0.000757\n",
            "Finish\n",
            "Train Epoch: 29 [0/320000 (0%)]\tLoss: 0.000745\n",
            "Train Epoch: 29 [200000/320000 (62%)]\tLoss: 0.000546\n",
            "Finish\n",
            "Train Epoch: 30 [0/320000 (0%)]\tLoss: 0.000144\n",
            "Train Epoch: 30 [200000/320000 (62%)]\tLoss: 0.000118\n",
            "Finish\n",
            "Train Epoch: 31 [0/320000 (0%)]\tLoss: 0.000951\n",
            "Train Epoch: 31 [200000/320000 (62%)]\tLoss: 0.000622\n",
            "Finish\n",
            "Train Epoch: 32 [0/320000 (0%)]\tLoss: 0.000520\n",
            "Train Epoch: 32 [200000/320000 (62%)]\tLoss: 0.000515\n",
            "Finish\n",
            "Train Epoch: 33 [0/320000 (0%)]\tLoss: 0.000716\n",
            "Train Epoch: 33 [200000/320000 (62%)]\tLoss: 0.000661\n",
            "Finish\n",
            "Train Epoch: 34 [0/320000 (0%)]\tLoss: 0.000233\n",
            "Train Epoch: 34 [200000/320000 (62%)]\tLoss: 0.000507\n",
            "Finish\n",
            "Train Epoch: 35 [0/320000 (0%)]\tLoss: 0.000534\n",
            "Train Epoch: 35 [200000/320000 (62%)]\tLoss: 0.000727\n",
            "Finish\n",
            "Train Epoch: 36 [0/320000 (0%)]\tLoss: 0.000094\n",
            "Train Epoch: 36 [200000/320000 (62%)]\tLoss: 0.000438\n",
            "Finish\n",
            "Train Epoch: 37 [0/320000 (0%)]\tLoss: 0.000270\n",
            "Train Epoch: 37 [200000/320000 (62%)]\tLoss: 0.001258\n",
            "Finish\n",
            "Train Epoch: 38 [0/320000 (0%)]\tLoss: 0.000311\n",
            "Train Epoch: 38 [200000/320000 (62%)]\tLoss: 0.000718\n",
            "Finish\n",
            "Train Epoch: 39 [0/320000 (0%)]\tLoss: 0.000398\n",
            "Train Epoch: 39 [200000/320000 (62%)]\tLoss: 0.001491\n",
            "Finish\n",
            "Train Epoch: 40 [0/320000 (0%)]\tLoss: 0.000686\n",
            "Train Epoch: 40 [200000/320000 (62%)]\tLoss: 0.000657\n",
            "Finish\n",
            "Train Epoch: 41 [0/320000 (0%)]\tLoss: 0.000576\n",
            "Train Epoch: 41 [200000/320000 (62%)]\tLoss: 0.000475\n",
            "Finish\n",
            "Train Epoch: 42 [0/320000 (0%)]\tLoss: 0.000483\n",
            "Train Epoch: 42 [200000/320000 (62%)]\tLoss: 0.000703\n",
            "Finish\n",
            "Train Epoch: 43 [0/320000 (0%)]\tLoss: 0.000556\n",
            "Train Epoch: 43 [200000/320000 (62%)]\tLoss: 0.000414\n",
            "Finish\n",
            "Train Epoch: 44 [0/320000 (0%)]\tLoss: 0.000362\n",
            "Train Epoch: 44 [200000/320000 (62%)]\tLoss: 0.000311\n",
            "Finish\n",
            "Train Epoch: 45 [0/320000 (0%)]\tLoss: 0.000361\n",
            "Train Epoch: 45 [200000/320000 (62%)]\tLoss: 0.000218\n",
            "Finish\n",
            "Train Epoch: 46 [0/320000 (0%)]\tLoss: 0.000770\n",
            "Train Epoch: 46 [200000/320000 (62%)]\tLoss: 0.000096\n",
            "Finish\n",
            "Train Epoch: 47 [0/320000 (0%)]\tLoss: 0.000578\n",
            "Train Epoch: 47 [200000/320000 (62%)]\tLoss: 0.000238\n",
            "Finish\n",
            "Train Epoch: 48 [0/320000 (0%)]\tLoss: 0.000101\n",
            "Train Epoch: 48 [200000/320000 (62%)]\tLoss: 0.001250\n",
            "Finish\n",
            "Train Epoch: 49 [0/320000 (0%)]\tLoss: 0.000778\n",
            "Train Epoch: 49 [200000/320000 (62%)]\tLoss: 0.000119\n",
            "Finish\n",
            "Train Epoch: 50 [0/320000 (0%)]\tLoss: 0.000432\n",
            "Train Epoch: 50 [200000/320000 (62%)]\tLoss: 0.000865\n",
            "Finish\n",
            "Train Epoch: 51 [0/320000 (0%)]\tLoss: 0.000095\n",
            "Train Epoch: 51 [200000/320000 (62%)]\tLoss: 0.000714\n",
            "Finish\n",
            "Train Epoch: 52 [0/320000 (0%)]\tLoss: 0.000936\n",
            "Train Epoch: 52 [200000/320000 (62%)]\tLoss: 0.000180\n",
            "Finish\n",
            "Train Epoch: 53 [0/320000 (0%)]\tLoss: 0.000495\n",
            "Train Epoch: 53 [200000/320000 (62%)]\tLoss: 0.000156\n",
            "Finish\n",
            "Train Epoch: 54 [0/320000 (0%)]\tLoss: 0.000295\n",
            "Train Epoch: 54 [200000/320000 (62%)]\tLoss: 0.001521\n",
            "Finish\n",
            "Train Epoch: 55 [0/320000 (0%)]\tLoss: 0.000299\n",
            "Train Epoch: 55 [200000/320000 (62%)]\tLoss: 0.000778\n",
            "Finish\n",
            "Train Epoch: 56 [0/320000 (0%)]\tLoss: 0.000455\n",
            "Train Epoch: 56 [200000/320000 (62%)]\tLoss: 0.000260\n",
            "Finish\n",
            "Train Epoch: 57 [0/320000 (0%)]\tLoss: 0.000556\n",
            "Train Epoch: 57 [200000/320000 (62%)]\tLoss: 0.000678\n",
            "Finish\n",
            "Train Epoch: 58 [0/320000 (0%)]\tLoss: 0.000793\n",
            "Train Epoch: 58 [200000/320000 (62%)]\tLoss: 0.000932\n",
            "Finish\n",
            "Train Epoch: 59 [0/320000 (0%)]\tLoss: 0.001180\n",
            "Train Epoch: 59 [200000/320000 (62%)]\tLoss: 0.001001\n",
            "Finish\n",
            "Train Epoch: 60 [0/320000 (0%)]\tLoss: 0.000525\n",
            "Train Epoch: 60 [200000/320000 (62%)]\tLoss: 0.000425\n",
            "Finish\n",
            "Train Epoch: 61 [0/320000 (0%)]\tLoss: 0.000343\n",
            "Train Epoch: 61 [200000/320000 (62%)]\tLoss: 0.000995\n",
            "Finish\n",
            "Train Epoch: 62 [0/320000 (0%)]\tLoss: 0.000823\n",
            "Train Epoch: 62 [200000/320000 (62%)]\tLoss: 0.000666\n",
            "Finish\n",
            "Train Epoch: 63 [0/320000 (0%)]\tLoss: 0.000085\n",
            "Train Epoch: 63 [200000/320000 (62%)]\tLoss: 0.000518\n",
            "Finish\n",
            "Train Epoch: 64 [0/320000 (0%)]\tLoss: 0.000122\n",
            "Train Epoch: 64 [200000/320000 (62%)]\tLoss: 0.000649\n",
            "Finish\n",
            "Train Epoch: 65 [0/320000 (0%)]\tLoss: 0.000291\n",
            "Train Epoch: 65 [200000/320000 (62%)]\tLoss: 0.000393\n",
            "Finish\n",
            "Train Epoch: 66 [0/320000 (0%)]\tLoss: 0.000301\n",
            "Train Epoch: 66 [200000/320000 (62%)]\tLoss: 0.000763\n",
            "Finish\n",
            "Train Epoch: 67 [0/320000 (0%)]\tLoss: 0.000426\n",
            "Train Epoch: 67 [200000/320000 (62%)]\tLoss: 0.000414\n",
            "Finish\n",
            "Train Epoch: 68 [0/320000 (0%)]\tLoss: 0.000618\n",
            "Train Epoch: 68 [200000/320000 (62%)]\tLoss: 0.000570\n",
            "Finish\n",
            "Train Epoch: 69 [0/320000 (0%)]\tLoss: 0.000314\n",
            "Train Epoch: 69 [200000/320000 (62%)]\tLoss: 0.000121\n",
            "Finish\n",
            "Train Epoch: 70 [0/320000 (0%)]\tLoss: 0.000320\n",
            "Train Epoch: 70 [200000/320000 (62%)]\tLoss: 0.000236\n",
            "Finish\n",
            "Train Epoch: 71 [0/320000 (0%)]\tLoss: 0.001049\n",
            "Train Epoch: 71 [200000/320000 (62%)]\tLoss: 0.000477\n",
            "Finish\n",
            "Train Epoch: 72 [0/320000 (0%)]\tLoss: 0.001394\n",
            "Train Epoch: 72 [200000/320000 (62%)]\tLoss: 0.000359\n",
            "Finish\n",
            "Train Epoch: 73 [0/320000 (0%)]\tLoss: 0.000103\n",
            "Train Epoch: 73 [200000/320000 (62%)]\tLoss: 0.000769\n",
            "Finish\n",
            "Train Epoch: 74 [0/320000 (0%)]\tLoss: 0.000697\n",
            "Train Epoch: 74 [200000/320000 (62%)]\tLoss: 0.000935\n",
            "Finish\n",
            "Train Epoch: 75 [0/320000 (0%)]\tLoss: 0.000102\n",
            "Train Epoch: 75 [200000/320000 (62%)]\tLoss: 0.000353\n",
            "Finish\n",
            "Train Epoch: 76 [0/320000 (0%)]\tLoss: 0.000552\n",
            "Train Epoch: 76 [200000/320000 (62%)]\tLoss: 0.001065\n",
            "Finish\n",
            "Train Epoch: 77 [0/320000 (0%)]\tLoss: 0.000295\n",
            "Train Epoch: 77 [200000/320000 (62%)]\tLoss: 0.000738\n",
            "Finish\n",
            "Train Epoch: 78 [0/320000 (0%)]\tLoss: 0.000408\n",
            "Train Epoch: 78 [200000/320000 (62%)]\tLoss: 0.000497\n",
            "Finish\n",
            "Train Epoch: 79 [0/320000 (0%)]\tLoss: 0.000321\n",
            "Train Epoch: 79 [200000/320000 (62%)]\tLoss: 0.000735\n",
            "Finish\n",
            "Train Epoch: 80 [0/320000 (0%)]\tLoss: 0.000510\n",
            "Train Epoch: 80 [200000/320000 (62%)]\tLoss: 0.000255\n",
            "Finish\n",
            "Train Epoch: 81 [0/320000 (0%)]\tLoss: 0.000529\n",
            "Train Epoch: 81 [200000/320000 (62%)]\tLoss: 0.000622\n",
            "Finish\n",
            "Train Epoch: 82 [0/320000 (0%)]\tLoss: 0.000087\n",
            "Train Epoch: 82 [200000/320000 (62%)]\tLoss: 0.000518\n",
            "Finish\n",
            "Train Epoch: 83 [0/320000 (0%)]\tLoss: 0.000600\n",
            "Train Epoch: 83 [200000/320000 (62%)]\tLoss: 0.000848\n",
            "Finish\n",
            "Train Epoch: 84 [0/320000 (0%)]\tLoss: 0.000787\n",
            "Train Epoch: 84 [200000/320000 (62%)]\tLoss: 0.000480\n",
            "Finish\n",
            "Train Epoch: 85 [0/320000 (0%)]\tLoss: 0.000077\n",
            "Train Epoch: 85 [200000/320000 (62%)]\tLoss: 0.000771\n",
            "Finish\n",
            "Train Epoch: 86 [0/320000 (0%)]\tLoss: 0.001874\n",
            "Train Epoch: 86 [200000/320000 (62%)]\tLoss: 0.000444\n",
            "Finish\n",
            "Train Epoch: 87 [0/320000 (0%)]\tLoss: 0.001339\n",
            "Train Epoch: 87 [200000/320000 (62%)]\tLoss: 0.000085\n",
            "Finish\n",
            "Train Epoch: 88 [0/320000 (0%)]\tLoss: 0.000375\n",
            "Train Epoch: 88 [200000/320000 (62%)]\tLoss: 0.001090\n",
            "Finish\n",
            "Train Epoch: 89 [0/320000 (0%)]\tLoss: 0.000349\n",
            "Train Epoch: 89 [200000/320000 (62%)]\tLoss: 0.000132\n",
            "Finish\n",
            "Train Epoch: 90 [0/320000 (0%)]\tLoss: 0.000053\n",
            "Train Epoch: 90 [200000/320000 (62%)]\tLoss: 0.000708\n",
            "Finish\n",
            "Train Epoch: 91 [0/320000 (0%)]\tLoss: 0.001140\n",
            "Train Epoch: 91 [200000/320000 (62%)]\tLoss: 0.000348\n",
            "Finish\n",
            "Train Epoch: 92 [0/320000 (0%)]\tLoss: 0.000542\n",
            "Train Epoch: 92 [200000/320000 (62%)]\tLoss: 0.000319\n",
            "Finish\n",
            "Train Epoch: 93 [0/320000 (0%)]\tLoss: 0.000482\n",
            "Train Epoch: 93 [200000/320000 (62%)]\tLoss: 0.001151\n",
            "Finish\n",
            "Train Epoch: 94 [0/320000 (0%)]\tLoss: 0.001161\n",
            "Train Epoch: 94 [200000/320000 (62%)]\tLoss: 0.000638\n",
            "Finish\n",
            "Train Epoch: 95 [0/320000 (0%)]\tLoss: 0.001463\n",
            "Train Epoch: 95 [200000/320000 (62%)]\tLoss: 0.000589\n",
            "Finish\n",
            "Train Epoch: 96 [0/320000 (0%)]\tLoss: 0.001138\n",
            "Train Epoch: 96 [200000/320000 (62%)]\tLoss: 0.000285\n",
            "Finish\n",
            "Train Epoch: 97 [0/320000 (0%)]\tLoss: 0.000269\n",
            "Train Epoch: 97 [200000/320000 (62%)]\tLoss: 0.001015\n",
            "Finish\n",
            "Train Epoch: 98 [0/320000 (0%)]\tLoss: 0.000482\n",
            "Train Epoch: 98 [200000/320000 (62%)]\tLoss: 0.000955\n",
            "Finish\n",
            "Train Epoch: 99 [0/320000 (0%)]\tLoss: 0.000333\n",
            "Train Epoch: 99 [200000/320000 (62%)]\tLoss: 0.001174\n",
            "Finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EWtgw4yaqU9Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 逆Feature Engineering"
      ]
    },
    {
      "metadata": {
        "id": "hXhzN39mWP4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def backtoNum(data,preprocess = True):\n",
        "  p = ''\n",
        "  if preprocess:\n",
        "    data = np.array(data)\n",
        "    if len(data)==48:\n",
        "      data = data.reshape(4,12)\n",
        "    elif len(data)==96:\n",
        "      data = data.reshape(8,12)\n",
        "    data = data.tolist()\n",
        "    \n",
        "  for i in range(len(data)):\n",
        "    k = data[i].index(max(data[i]))\n",
        "    if k == 0:\n",
        "      a = ' '\n",
        "    elif k == 1:\n",
        "      a = '+'\n",
        "    else:\n",
        "      a = format(k-2)\n",
        "    p = p+a\n",
        "  return(p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MnQwVur_qe8f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 測試結果"
      ]
    },
    {
      "metadata": {
        "id": "jo1D-_p6_X5W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test():\n",
        "  model.eval()\n",
        "  correct = False\n",
        "  c = 0\n",
        "  for step, (x, y) in enumerate(test_dataloader):\n",
        "    data = x.type(torch.cuda.FloatTensor)\n",
        "    target = y.type(torch.cuda.FloatTensor)\n",
        "\n",
        "    output = model(data)\n",
        "    #print(output)\n",
        "\n",
        "\n",
        "    da = backtoNum(data[0].cpu().detach().numpy())\n",
        "    an = backtoNum(output[0].cpu().detach().numpy())\n",
        "    co = backtoNum(target[0].cpu().detach().numpy())\n",
        "\n",
        "    if an==co:\n",
        "      correct = True\n",
        "      c = c +1\n",
        "    else:\n",
        "      correct = False\n",
        "\n",
        "    if step<20:\n",
        "      print(da+\" ?= \"+an+' '+format(correct)+' '+co)\n",
        "      \n",
        "  p = c*100/len(test_dataloader)\n",
        "  print(\"Accuracy : \"+format(c)+'/'+format(len(test_dataloader))+' = '+format(p)+'%')\n",
        "  print(\"Finish\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XzcuVbx0usYx",
        "colab_type": "code",
        "outputId": "c2e6d068-d3a2-4b13-dc81-4947d28d2150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99+355   ?= 454  True 454 \n",
            "975+867  ?= 1842 True 1842\n",
            "782+539  ?= 1321 True 1321\n",
            "538+706  ?= 1244 True 1244\n",
            "41+149   ?= 190  True 190 \n",
            "656+83   ?= 739  True 739 \n",
            "564+953  ?= 1517 True 1517\n",
            "44+619   ?= 663  True 663 \n",
            "316+289  ?= 605  True 605 \n",
            "400+94   ?= 494  True 494 \n",
            "876+567  ?= 1443 True 1443\n",
            "933+314  ?= 1247 True 1247\n",
            "426+578  ?= 1004 True 1004\n",
            "492+415  ?= 907  True 907 \n",
            "77+767   ?= 844  True 844 \n",
            "680+944  ?= 1624 True 1624\n",
            "153+981  ?= 1134 True 1134\n",
            "753+943  ?= 1696 True 1696\n",
            "93+572   ?= 665  True 665 \n",
            "197+935  ?= 1132 True 1132\n",
            "Accuracy : 78718/80000 = 98.3975%\n",
            "Finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M-ZPgAjzqmMi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}